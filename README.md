# Advanced Machine Learning Project (Fall 2024 - ENSAE Paris) - Wasserstein Generative Adversarial Networks

**Generative modeling** encompasses a class of machine learning techniques designed to capture the underlying distribution of a dataset, enabling the generation of new samples that closely resemble the original data. This constitutes an unsupervised learning problem that fundamentally consists—explicitly or implicitly—in min- imizing a divergence metric between the unknown true data distribution and the an estimated distribution to produce realistic samples. Generative Adversarial Networks (GANs) ([Goodfellow et al.](https://arxiv.org/abs/1406.2661)), by capitalizing on the powerful representational capabilities of deep learning architectures, have represented one of the most significant machine learning breakthroughs in recent years. Yet, this method comes with drawbacks that justify the present project.

**Contributions**. Throughout this work, the various practical challenges inherent in training GANs will be empirically explored and theoretically justified. Building on the work of [Arkovsky et al.](https://arxiv.org/abs/1701.07875) we will demonstrate how these challenges can be mitigated through Wasserstein Generative Adversarial Networks (WGANs), supported by rigorous theoretical justifications. 

**Authors**: Naïl Khelifa - Tom Rossa
---

## Structure

1. In the `src` folder, there are three .py files:
   - `train.py`: defines the code for training the architectures specified in the `models` folder.
   - `main.py`: contains the complete pipeline for training the models by calling the functions in `train.py`.
   - `mode_collapse.py`: contains the complete pipeline to carry out the mode collapsing experiment, implemented in the `mode_collapse.ipynb` notebook.

2. In the `ntbk` folder, there are three notebooks:
   - `main.ipynb`: where the training experiment for `main.py` is conducted.
   - `mode_collapse.ipynb`: where the training experiment for `mode_collapse.py` is conducted.
   - (optional) `toy_example.ipynb` and `toy_example2.ipynb`: where two naive experiments are conducted, which were not included in the final report.

3. In the `models` folder, there are the architectures corresponding to the models trained in `train.py`, `main.py`, and `main.ipynb`:
   - `dcgan.py`: for Deep Convolutional GAN.
   - `wgan_cp.py`: for Wasserstein GAN - Weight Clipping.
   - `wgan_gp.py`: for Wasserstein GAN - Gradient Penalty.

4. In the `imgs` folder, there are images generated by the generators during training.

5. In `trained_models`, there are the `state_dict` dictionaries corresponding to the trained and saved models.

## Running the code

To run the code, execute `init.ipynb` (the first cell) and then navigate through the corresponding notebooks.

   
